{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20180418: for bird_tw(73:1812), frog_tw(35:246), dog(15:66). (classNum:fileNum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "import sys, getopt\n",
    "import datetime\n",
    "from io_utils_mod import HDF5Matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# from MapCallback import MapCallback\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5path = '../data/all_data.hdf5' # training data generated by loadData.py\n",
    "modelName2Save = 'trainModel_InceptionV3_1-v5_bird_frog_dog.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5path: ../data/all_data.hdf5, scalerFilePath: None\n"
     ]
    }
   ],
   "source": [
    "modelPath = 'InceptionV3' #M: 沒用\n",
    "logfileName = 'log.xls' #M: 沒用\n",
    "#scalerFilePath = '../birdclef_data/standardScaler_5000.pickle'\n",
    "scalerFilePath = None\n",
    "preTrainedModelWeightsPath = None # path and filename to pretrained network: if there is a pretrained network, we can load it and continue to train it\n",
    "tensorflowBackend = False # set true if Keras has TensorFlow backend - this way we set TF not to allocate all the GPU memory\n",
    "\n",
    "if (tensorflowBackend):\n",
    "    import tensorflow as tf\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    from keras import backend as K\n",
    "    K.set_session(sess)\n",
    "\n",
    "print('hdf5path: %s, scalerFilePath: %s' % (hdf5path,scalerFilePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "scaleData = None\n",
    "# if a scaler file generated by loadData.py is given, than load it and define a scaler function that will be used later\n",
    "if scalerFilePath is not None:\n",
    "    scaler = pickle.load(open(scalerFilePath, 'rb'))\n",
    "    # Can't use scaler.transform because it only supports 2d arrays.\n",
    "    def scaleData(X):\n",
    "        return (X-scaler.mean_)/scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (18308, 1, 200, 310)\n",
      "dataSetLength=18308\n",
      "output_dim=123\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(hdf5path, 'r')\n",
    "X = f.get('X')\n",
    "y = f.get('y')\n",
    "print(\"Shape of X:\", X.shape)\n",
    "dataSetLength = X.shape[0]\n",
    "output_dim = y.shape[1] #len(y_train[0])\n",
    "f.close()\n",
    "print('dataSetLength={}'.format(dataSetLength))\n",
    "print('output_dim={}'.format(output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(HDF5Matrix(hdf5path, 'X', 0, dataSetLength, normalizer = scaleData))\n",
    "y = np.array(HDF5Matrix(hdf5path, 'y', 0, dataSetLength))\n",
    "\n",
    "#M: vvv 再多 shuffle 幾次...\n",
    "X, y = shuffle(X, y, random_state=4)\n",
    "X, y = shuffle(X, y)\n",
    "X, y = shuffle(X, y)\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X_train, y_train= (14829, 1, 200, 310) (14829, 123) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "   X_validation, y_validation= (1648, 1, 200, 310) (1648, 123)\n",
      "   X_test, y_test= (1831, 1, 200, 310) (1831, 123)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 4, test_size = 0.1)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, random_state = 3, test_size = 0.1)\n",
    "\n",
    "print('   X_train, y_train=', X_train.shape, y_train.shape, type(X_train), type(y_train))\n",
    "print('   X_validation, y_validation=', X_validation.shape, y_validation.shape)\n",
    "print('   X_test, y_test=', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c3f506198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#M: 隨便抽張圖看看...\n",
    "i=1\n",
    "img = X_train[i].reshape((200,310))\n",
    "fig = plt.figure(figsize=(19,10))\n",
    "plt.tight_layout()\n",
    "plt.pcolormesh(img, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vvvvv 將 AlexNet 換成 InceptionV3 Model vvvvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adam, Nadam #M: 用 Adam, Nadam 沒用.\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "model = InceptionV3(include_top=False, weights=None, input_shape=(1,200,310), pooling='avg')\n",
    "\n",
    "input_tensor = model.input\n",
    "# build top\n",
    "x = model.output\n",
    "#M: 多加一層 128 dense layer\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(output_dim, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=x)\n",
    "\n",
    "model.compile(optimizer=Nadam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 200, 310)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 99, 154)  288         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 99, 154)  96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 99, 154)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 97, 152)  9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 97, 152)  96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 97, 152)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 97, 152)  18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 97, 152)  192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 97, 152)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 48, 75)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 80, 48, 75)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 80, 48, 75)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 80, 48, 75)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 192, 46, 73)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 192, 46, 73)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 192, 46, 73)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 192, 22, 36)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 22, 36)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 22, 36)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 22, 36)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 48, 22, 36)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 96, 22, 36)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 48, 22, 36)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 96, 22, 36)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 48, 22, 36)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 96, 22, 36)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 192, 22, 36)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 22, 36)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 22, 36)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 96, 22, 36)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 22, 36)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 22, 36)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 22, 36)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 96, 22, 36)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 22, 36)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 22, 36)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 22, 36)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 96, 22, 36)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 22, 36)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 256, 22, 36)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 22, 36)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 22, 36)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 22, 36)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 48, 22, 36)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 96, 22, 36)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 48, 22, 36)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 96, 22, 36)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 48, 22, 36)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 96, 22, 36)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 256, 22, 36)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 22, 36)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 22, 36)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 96, 22, 36)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 22, 36)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 22, 36)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 22, 36)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 96, 22, 36)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 22, 36)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 22, 36)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 22, 36)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 96, 22, 36)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 22, 36)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 288, 22, 36)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, 22, 36)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 64, 22, 36)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, 22, 36)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 48, 22, 36)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 96, 22, 36)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 48, 22, 36)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 96, 22, 36)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 48, 22, 36)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 96, 22, 36)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 288, 22, 36)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 22, 36)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 22, 36)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 96, 22, 36)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, 22, 36)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 64, 22, 36)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 64, 22, 36)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 96, 22, 36)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 22, 36)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 22, 36)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 22, 36)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 96, 22, 36)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64, 22, 36)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 288, 22, 36)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 64, 22, 36)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, 22, 36)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64, 22, 36)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 96, 22, 36)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 96, 22, 36)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 96, 22, 36)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 384, 10, 17)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 96, 10, 17)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 384, 10, 17)  1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 96, 10, 17)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 384, 10, 17)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 96, 10, 17)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 288, 10, 17)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 768, 10, 17)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 128, 10, 17)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 128, 10, 17)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 128, 10, 17)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 128, 10, 17)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 128, 10, 17)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 128, 10, 17)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 10, 17)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 128, 10, 17)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 128, 10, 17)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 128, 10, 17)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 128, 10, 17)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 128, 10, 17)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 128, 10, 17)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 128, 10, 17)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 128, 10, 17)  384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 128, 10, 17)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 128, 10, 17)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 128, 10, 17)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 768, 10, 17)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 192, 10, 17)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 192, 10, 17)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 192, 10, 17)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 192, 10, 17)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 192, 10, 17)  576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 192, 10, 17)  576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 192, 10, 17)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 192, 10, 17)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 192, 10, 17)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 192, 10, 17)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 192, 10, 17)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 192, 10, 17)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 768, 10, 17)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 160, 10, 17)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 160, 10, 17)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 160, 10, 17)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 160, 10, 17)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 160, 10, 17)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 160, 10, 17)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 160, 10, 17)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 160, 10, 17)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 160, 10, 17)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 160, 10, 17)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 160, 10, 17)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 160, 10, 17)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 160, 10, 17)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 160, 10, 17)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 160, 10, 17)  480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 160, 10, 17)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 160, 10, 17)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 160, 10, 17)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 768, 10, 17)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 192, 10, 17)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 192, 10, 17)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 192, 10, 17)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 192, 10, 17)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 192, 10, 17)  576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 192, 10, 17)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 192, 10, 17)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 192, 10, 17)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 192, 10, 17)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 192, 10, 17)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 192, 10, 17)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 192, 10, 17)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 768, 10, 17)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 160, 10, 17)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 160, 10, 17)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 160, 10, 17)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 160, 10, 17)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 160, 10, 17)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 160, 10, 17)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 160, 10, 17)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 160, 10, 17)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 160, 10, 17)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 160, 10, 17)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 160, 10, 17)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 160, 10, 17)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 160, 10, 17)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 160, 10, 17)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 160, 10, 17)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 160, 10, 17)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 160, 10, 17)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 160, 10, 17)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 768, 10, 17)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 192, 10, 17)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 192, 10, 17)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 192, 10, 17)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 192, 10, 17)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 192, 10, 17)  576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 192, 10, 17)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 192, 10, 17)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 192, 10, 17)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 192, 10, 17)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 192, 10, 17)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 192, 10, 17)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 192, 10, 17)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 768, 10, 17)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 192, 10, 17)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 192, 10, 17)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 192, 10, 17)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 192, 10, 17)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 192, 10, 17)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 192, 10, 17)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 192, 10, 17)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 192, 10, 17)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 192, 10, 17)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 192, 10, 17)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 192, 10, 17)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 192, 10, 17)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 192, 10, 17)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 192, 10, 17)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 192, 10, 17)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 192, 10, 17)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 192, 10, 17)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 192, 10, 17)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 768, 10, 17)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 192, 10, 17)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 192, 10, 17)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 192, 10, 17)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 192, 10, 17)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 192, 10, 17)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 192, 10, 17)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 192, 10, 17)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 192, 10, 17)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 192, 10, 17)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 192, 10, 17)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 192, 10, 17)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 192, 10, 17)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 768, 10, 17)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 192, 10, 17)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 192, 10, 17)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 192, 10, 17)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 192, 10, 17)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 192, 10, 17)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 192, 10, 17)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 192, 10, 17)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 192, 10, 17)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 192, 10, 17)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 192, 10, 17)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 192, 10, 17)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 192, 10, 17)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 320, 4, 8)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 192, 4, 8)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 320, 4, 8)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 192, 4, 8)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 320, 4, 8)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 192, 4, 8)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 768, 4, 8)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 1280, 4, 8)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 448, 4, 8)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 448, 4, 8)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 448, 4, 8)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 384, 4, 8)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 384, 4, 8)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 384, 4, 8)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 384, 4, 8)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 384, 4, 8)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 384, 4, 8)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 384, 4, 8)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 384, 4, 8)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 384, 4, 8)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 384, 4, 8)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 1280, 4, 8)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 320, 4, 8)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 384, 4, 8)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 384, 4, 8)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 384, 4, 8)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 384, 4, 8)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 192, 4, 8)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 320, 4, 8)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 384, 4, 8)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 384, 4, 8)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 384, 4, 8)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 384, 4, 8)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 192, 4, 8)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 320, 4, 8)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 768, 4, 8)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768, 4, 8)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 192, 4, 8)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 2048, 4, 8)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 448, 4, 8)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 448, 4, 8)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 448, 4, 8)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 384, 4, 8)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 384, 4, 8)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 384, 4, 8)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 384, 4, 8)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 384, 4, 8)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 384, 4, 8)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 384, 4, 8)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 384, 4, 8)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 384, 4, 8)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 384, 4, 8)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 2048, 4, 8)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 320, 4, 8)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 384, 4, 8)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 384, 4, 8)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 384, 4, 8)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 384, 4, 8)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 192, 4, 8)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 320, 4, 8)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 384, 4, 8)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 384, 4, 8)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 384, 4, 8)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 384, 4, 8)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 192, 4, 8)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 320, 4, 8)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 768, 4, 8)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 768, 4, 8)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 192, 4, 8)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 2048, 4, 8)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          262272      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 123)          15867       dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 22,080,347\n",
      "Trainable params: 22,045,915\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^^^^^ 將 AlexNet 換成 InceptionV3 Model ^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 20000 # number of epochs, should be high, the end of the learning process is controled by early stoping\n",
    "es_patience = 150 # patience for early stoping \n",
    "batchSize = 100 # batch size for mini-batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr decay schedule\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    lr = 1e-3\n",
    "    decay = int(epoch / 100)\n",
    "    if decay != 0:\n",
    "        lr /= (10 ** decay)\n",
    "    print('Learning rate = {}, decay = {}'.format(lr, decay))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call backs\n",
    "if os.path.exists('./modelWeights/') is False:\n",
    "    os.mkdir('./modelWeights/') #M:\n",
    "checkpointer = ModelCheckpoint(filepath='./modelWeights/weights.h5', verbose=1, save_best_only=True)\n",
    "lr = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and compile it, we use RMSprop here, other optimizer algorithm should be tested\n",
    "# execfile(modelPath)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')#, metrics=[\"accuracy\"])\n",
    "\n",
    "# print the model\n",
    "# print(\"The following model is used: \")\n",
    "# for layer in model.layers:\n",
    "#     print(\"{} output shape: {}\".format(layer.name, layer.output_shape))\n",
    "\n",
    "# load pretrained model if it is set\n",
    "# if preTrainedModelWeightsPath is not None:\n",
    "#     model.load_weights(preTrainedModelWeightsPath)\n",
    "#     print(\"Reloaded weights from: {}\".format(preTrainedModelWeightsPath))\n",
    "\n",
    "# define callback functions\n",
    "# mapcallback = MapCallback()\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience = es_patience) # early stoping\n",
    "\n",
    "# save best models based on accuracy, loss and MAP metrics\n",
    "#bestModelFilePath_val_map = './modelWeights/best_val_map_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))\n",
    "#bestModelFilePath_val_acc = './modelWeights/best_val_acc_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))\n",
    "#bestModelFilePath_val_loss = './modelWeights/best_val_loss_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))\n",
    "\n",
    "\n",
    "# bestModelFilePath_val_acc = './modelWeights/best_val_acc_{}.hdf5'.format(output_dim)\n",
    "# bestModelFilePath_val_loss = './modelWeights/best_val_loss_{}.hdf5'.format(output_dim)\n",
    "# bestModelFilePath_val_map = './modelWeights/best_val_map_{}.hdf5'.format(output_dim)\n",
    "# checkpointer_val_acc = ModelCheckpoint(filepath = bestModelFilePath_val_acc, verbose = 1, monitor = 'val_acc', save_best_only = True)\n",
    "# checkpointer_val_loss = ModelCheckpoint(filepath = bestModelFilePath_val_loss, verbose = 1, monitor = 'val_loss', save_best_only = True)\n",
    "# checkpointer_val_map = ModelCheckpoint(filepath = bestModelFilePath_val_map, verbose = 1, monitor = 'val_map', mode = 'max', save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14829 samples, validate on 1648 samples\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 1/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 4.1751 - acc: 0.1349\n",
      "Epoch 00001: val_loss improved from inf to 14.67521, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 126s 8ms/step - loss: 4.1740 - acc: 0.1351 - val_loss: 14.6752 - val_acc: 0.0158\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 2/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 3.5444 - acc: 0.2064\n",
      "Epoch 00002: val_loss improved from 14.67521 to 4.77551, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 83s 6ms/step - loss: 3.5452 - acc: 0.2064 - val_loss: 4.7755 - val_acc: 0.0176\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 3/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 3.2272 - acc: 0.2528\n",
      "Epoch 00003: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 3.2269 - acc: 0.2527 - val_loss: 9.4549 - val_acc: 0.0206\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 4/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 2.9512 - acc: 0.3035\n",
      "Epoch 00004: val_loss improved from 4.77551 to 3.87922, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 83s 6ms/step - loss: 2.9518 - acc: 0.3035 - val_loss: 3.8792 - val_acc: 0.2354\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 5/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 2.6217 - acc: 0.3618\n",
      "Epoch 00005: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 2.6205 - acc: 0.3619 - val_loss: 5.6330 - val_acc: 0.2184\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 6/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 2.3635 - acc: 0.4149\n",
      "Epoch 00006: val_loss improved from 3.87922 to 3.06119, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 82s 6ms/step - loss: 2.3631 - acc: 0.4150 - val_loss: 3.0612 - val_acc: 0.3580\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 7/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 2.1731 - acc: 0.4540\n",
      "Epoch 00007: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 2.1749 - acc: 0.4538 - val_loss: 4.0167 - val_acc: 0.2518\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 8/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 1.9736 - acc: 0.4968\n",
      "Epoch 00008: val_loss improved from 3.06119 to 2.91778, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 82s 6ms/step - loss: 1.9738 - acc: 0.4967 - val_loss: 2.9178 - val_acc: 0.4150\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 9/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 1.8190 - acc: 0.5315\n",
      "Epoch 00009: val_loss improved from 2.91778 to 2.67610, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 84s 6ms/step - loss: 1.8181 - acc: 0.5317 - val_loss: 2.6761 - val_acc: 0.4308\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 10/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 1.6464 - acc: 0.5725\n",
      "Epoch 00010: val_loss improved from 2.67610 to 2.28524, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 83s 6ms/step - loss: 1.6479 - acc: 0.5725 - val_loss: 2.2852 - val_acc: 0.4751\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 11/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 1.5515 - acc: 0.5970\n",
      "Epoch 00011: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 1.5518 - acc: 0.5968 - val_loss: 2.7123 - val_acc: 0.4320\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 12/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 1.4016 - acc: 0.6291\n",
      "Epoch 00012: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 1.4033 - acc: 0.6285 - val_loss: 4.6831 - val_acc: 0.2998\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 13/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 1.3085 - acc: 0.6554\n",
      "Epoch 00013: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 1.3084 - acc: 0.6552 - val_loss: 2.8237 - val_acc: 0.4763\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 14/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 1.1910 - acc: 0.6824\n",
      "Epoch 00014: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 1.1914 - acc: 0.6822 - val_loss: 3.0142 - val_acc: 0.4654\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 15/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 1.1013 - acc: 0.7022\n",
      "Epoch 00015: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 1.1020 - acc: 0.7019 - val_loss: 2.3397 - val_acc: 0.5407\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 16/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 1.0478 - acc: 0.7170\n",
      "Epoch 00016: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 1.0494 - acc: 0.7165 - val_loss: 3.4477 - val_acc: 0.4927\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 17/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.9663 - acc: 0.7375\n",
      "Epoch 00017: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.9678 - acc: 0.7372 - val_loss: 3.5853 - val_acc: 0.4654\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 18/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.8974 - acc: 0.7599\n",
      "Epoch 00018: val_loss improved from 2.28524 to 1.65520, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 81s 5ms/step - loss: 0.8983 - acc: 0.7597 - val_loss: 1.6552 - val_acc: 0.6262\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 19/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.7992 - acc: 0.7787\n",
      "Epoch 00019: val_loss improved from 1.65520 to 1.28327, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 86s 6ms/step - loss: 0.7988 - acc: 0.7788 - val_loss: 1.2833 - val_acc: 0.7027\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 20/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.7486 - acc: 0.7931\n",
      "Epoch 00020: val_loss did not improve\n",
      "14829/14829 [==============================] - 76s 5ms/step - loss: 0.7483 - acc: 0.7932 - val_loss: 1.3685 - val_acc: 0.6705\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 21/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.6869 - acc: 0.8096\n",
      "Epoch 00021: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.6862 - acc: 0.8098 - val_loss: 2.1192 - val_acc: 0.5941\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 22/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.6382 - acc: 0.8241\n",
      "Epoch 00022: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.6380 - acc: 0.8241 - val_loss: 1.3608 - val_acc: 0.7027\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 23/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.5992 - acc: 0.8353\n",
      "Epoch 00023: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.6006 - acc: 0.8349 - val_loss: 2.4990 - val_acc: 0.5376\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 24/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.8501\n",
      "Epoch 00024: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.5368 - acc: 0.8498 - val_loss: 1.3388 - val_acc: 0.7063\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 25/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.8575\n",
      "Epoch 00025: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.5105 - acc: 0.8572 - val_loss: 1.3129 - val_acc: 0.7269\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 26/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.4819 - acc: 0.8641\n",
      "Epoch 00026: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.4818 - acc: 0.8641 - val_loss: 1.7363 - val_acc: 0.6638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 27/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.4831 - acc: 0.8659\n",
      "Epoch 00027: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.4827 - acc: 0.8659 - val_loss: 1.8194 - val_acc: 0.6547\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 28/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.8736\n",
      "Epoch 00028: val_loss improved from 1.28327 to 1.24839, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 81s 5ms/step - loss: 0.4576 - acc: 0.8738 - val_loss: 1.2484 - val_acc: 0.7640\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 29/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8841\n",
      "Epoch 00029: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.4166 - acc: 0.8841 - val_loss: 1.2639 - val_acc: 0.7476\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 30/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8902\n",
      "Epoch 00030: val_loss improved from 1.24839 to 1.14280, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 81s 5ms/step - loss: 0.3853 - acc: 0.8903 - val_loss: 1.1428 - val_acc: 0.7640\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 31/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8980\n",
      "Epoch 00031: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.3610 - acc: 0.8979 - val_loss: 1.5432 - val_acc: 0.6990\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 32/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.9020\n",
      "Epoch 00032: val_loss improved from 1.14280 to 0.94234, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 81s 5ms/step - loss: 0.3552 - acc: 0.9019 - val_loss: 0.9423 - val_acc: 0.8022\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 33/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.9090\n",
      "Epoch 00033: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.3292 - acc: 0.9090 - val_loss: 1.2776 - val_acc: 0.7506\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 34/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2981 - acc: 0.9178\n",
      "Epoch 00034: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2976 - acc: 0.9179 - val_loss: 1.1342 - val_acc: 0.7876\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 35/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9176\n",
      "Epoch 00035: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2974 - acc: 0.9176 - val_loss: 1.2832 - val_acc: 0.7470\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 36/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9145\n",
      "Epoch 00036: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2993 - acc: 0.9142 - val_loss: 1.4349 - val_acc: 0.7512\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 37/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9150\n",
      "Epoch 00037: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.3037 - acc: 0.9151 - val_loss: 1.0718 - val_acc: 0.7882\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 38/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9286\n",
      "Epoch 00038: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2501 - acc: 0.9284 - val_loss: 1.3720 - val_acc: 0.7549\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 39/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9284\n",
      "Epoch 00039: val_loss improved from 0.94234 to 0.94220, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 81s 5ms/step - loss: 0.2483 - acc: 0.9284 - val_loss: 0.9422 - val_acc: 0.8246\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 40/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9350\n",
      "Epoch 00040: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2316 - acc: 0.9349 - val_loss: 1.1373 - val_acc: 0.7931\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 41/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9341\n",
      "Epoch 00041: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2375 - acc: 0.9342 - val_loss: 1.2288 - val_acc: 0.7718\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 42/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9407\n",
      "Epoch 00042: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2040 - acc: 0.9405 - val_loss: 1.4022 - val_acc: 0.7670\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 43/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9268\n",
      "Epoch 00043: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2679 - acc: 0.9268 - val_loss: 1.6372 - val_acc: 0.7257\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 44/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9401\n",
      "Epoch 00044: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2197 - acc: 0.9400 - val_loss: 1.1373 - val_acc: 0.7955\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 45/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9382\n",
      "Epoch 00045: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2221 - acc: 0.9382 - val_loss: 1.1722 - val_acc: 0.7913\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 46/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9443\n",
      "Epoch 00046: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2047 - acc: 0.9442 - val_loss: 0.9874 - val_acc: 0.8125\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 47/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9411\n",
      "Epoch 00047: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.2133 - acc: 0.9411 - val_loss: 2.1396 - val_acc: 0.6869\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 48/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9522\n",
      "Epoch 00048: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1645 - acc: 0.9522 - val_loss: 1.6579 - val_acc: 0.7379\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 49/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9544\n",
      "Epoch 00049: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1662 - acc: 0.9543 - val_loss: 1.1840 - val_acc: 0.7870\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 50/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9566\n",
      "Epoch 00050: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1593 - acc: 0.9565 - val_loss: 1.5231 - val_acc: 0.7379\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 51/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9466\n",
      "Epoch 00051: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1964 - acc: 0.9467 - val_loss: 1.5116 - val_acc: 0.7646\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 52/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9509\n",
      "Epoch 00052: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1755 - acc: 0.9508 - val_loss: 1.1732 - val_acc: 0.8192\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 53/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9621\n",
      "Epoch 00053: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1351 - acc: 0.9620 - val_loss: 0.9666 - val_acc: 0.8289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 54/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9516\n",
      "Epoch 00054: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1874 - acc: 0.9516 - val_loss: 1.8356 - val_acc: 0.6893\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 55/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9589\n",
      "Epoch 00055: val_loss improved from 0.94220 to 0.84948, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 81s 5ms/step - loss: 0.1592 - acc: 0.9589 - val_loss: 0.8495 - val_acc: 0.8356\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 56/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9596\n",
      "Epoch 00056: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1510 - acc: 0.9595 - val_loss: 1.4560 - val_acc: 0.7348\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 57/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9496\n",
      "Epoch 00057: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1933 - acc: 0.9495 - val_loss: 1.2309 - val_acc: 0.7925\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 58/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9674\n",
      "Epoch 00058: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1146 - acc: 0.9674 - val_loss: 1.0310 - val_acc: 0.8143\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 59/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9507\n",
      "Epoch 00059: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1896 - acc: 0.9507 - val_loss: 1.5192 - val_acc: 0.7706\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 60/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9658\n",
      "Epoch 00060: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1316 - acc: 0.9657 - val_loss: 1.0109 - val_acc: 0.8228\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 61/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9625\n",
      "Epoch 00061: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1429 - acc: 0.9625 - val_loss: 1.0002 - val_acc: 0.8331\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 62/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9661\n",
      "Epoch 00062: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1287 - acc: 0.9661 - val_loss: 0.9433 - val_acc: 0.8258\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 63/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9673\n",
      "Epoch 00063: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1239 - acc: 0.9674 - val_loss: 1.0884 - val_acc: 0.8307\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 64/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9683\n",
      "Epoch 00064: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1180 - acc: 0.9683 - val_loss: 1.8251 - val_acc: 0.7239\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 65/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9719\n",
      "Epoch 00065: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1046 - acc: 0.9717 - val_loss: 1.3936 - val_acc: 0.7488\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 66/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9609\n",
      "Epoch 00066: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1465 - acc: 0.9608 - val_loss: 1.1680 - val_acc: 0.7919\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 67/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9661\n",
      "Epoch 00067: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1201 - acc: 0.9659 - val_loss: 1.2609 - val_acc: 0.7919\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 68/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9668\n",
      "Epoch 00068: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1226 - acc: 0.9668 - val_loss: 1.2735 - val_acc: 0.8131\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 69/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9676\n",
      "Epoch 00069: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1277 - acc: 0.9674 - val_loss: 1.1204 - val_acc: 0.8113\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 70/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9618\n",
      "Epoch 00070: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1450 - acc: 0.9618 - val_loss: 1.4161 - val_acc: 0.7524\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 71/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9603\n",
      "Epoch 00071: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1563 - acc: 0.9604 - val_loss: 0.9227 - val_acc: 0.8325\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 72/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9759\n",
      "Epoch 00072: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0881 - acc: 0.9759 - val_loss: 1.1164 - val_acc: 0.8198\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 73/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9786\n",
      "Epoch 00073: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0790 - acc: 0.9786 - val_loss: 1.8285 - val_acc: 0.7294\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 74/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9750\n",
      "Epoch 00074: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0940 - acc: 0.9750 - val_loss: 1.9463 - val_acc: 0.7239\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 75/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9656\n",
      "Epoch 00075: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1359 - acc: 0.9655 - val_loss: 1.5990 - val_acc: 0.7500\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 76/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9640\n",
      "Epoch 00076: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1322 - acc: 0.9640 - val_loss: 0.8875 - val_acc: 0.8483\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 77/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9679\n",
      "Epoch 00077: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1244 - acc: 0.9678 - val_loss: 1.3204 - val_acc: 0.7894\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 78/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9639\n",
      "Epoch 00078: val_loss improved from 0.84948 to 0.73426, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 82s 6ms/step - loss: 0.1439 - acc: 0.9639 - val_loss: 0.7343 - val_acc: 0.8744\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 79/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9817\n",
      "Epoch 00079: val_loss did not improve\n",
      "14829/14829 [==============================] - 78s 5ms/step - loss: 0.0720 - acc: 0.9816 - val_loss: 0.8923 - val_acc: 0.8550\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 80/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9746\n",
      "Epoch 00080: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0971 - acc: 0.9745 - val_loss: 1.2443 - val_acc: 0.8277\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 81/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9747\n",
      "Epoch 00081: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0968 - acc: 0.9748 - val_loss: 1.3286 - val_acc: 0.7900\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 82/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9730\n",
      "Epoch 00082: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0969 - acc: 0.9730 - val_loss: 1.7313 - val_acc: 0.7342\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 83/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9722\n",
      "Epoch 00083: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1097 - acc: 0.9719 - val_loss: 1.4098 - val_acc: 0.7961\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 84/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9672\n",
      "Epoch 00084: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1316 - acc: 0.9671 - val_loss: 0.9386 - val_acc: 0.8313\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 85/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9761\n",
      "Epoch 00085: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0908 - acc: 0.9759 - val_loss: 1.1694 - val_acc: 0.8155\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 86/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9768\n",
      "Epoch 00086: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0877 - acc: 0.9767 - val_loss: 0.9030 - val_acc: 0.8483\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 87/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9804\n",
      "Epoch 00087: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0755 - acc: 0.9804 - val_loss: 1.1197 - val_acc: 0.8113\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 88/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9692\n",
      "Epoch 00088: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1344 - acc: 0.9691 - val_loss: 1.6746 - val_acc: 0.7427\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 89/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9746\n",
      "Epoch 00089: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0973 - acc: 0.9745 - val_loss: 0.8221 - val_acc: 0.8471\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 90/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9782\n",
      "Epoch 00090: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0916 - acc: 0.9782 - val_loss: 0.9604 - val_acc: 0.8362\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 91/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9850\n",
      "Epoch 00091: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0531 - acc: 0.9850 - val_loss: 1.0905 - val_acc: 0.8222\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 92/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9808\n",
      "Epoch 00092: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0777 - acc: 0.9808 - val_loss: 1.0348 - val_acc: 0.8301\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 93/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9816\n",
      "Epoch 00093: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0693 - acc: 0.9816 - val_loss: 1.0789 - val_acc: 0.8295\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 94/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9728\n",
      "Epoch 00094: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1093 - acc: 0.9726 - val_loss: 1.1387 - val_acc: 0.8143\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 95/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9732\n",
      "Epoch 00095: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1047 - acc: 0.9732 - val_loss: 1.2278 - val_acc: 0.7973\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 96/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9757\n",
      "Epoch 00096: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0986 - acc: 0.9758 - val_loss: 0.9786 - val_acc: 0.8410\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 97/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9754\n",
      "Epoch 00097: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0929 - acc: 0.9754 - val_loss: 1.2636 - val_acc: 0.8131\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 98/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9743\n",
      "Epoch 00098: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.1005 - acc: 0.9743 - val_loss: 0.9650 - val_acc: 0.8343\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 99/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9819\n",
      "Epoch 00099: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0725 - acc: 0.9819 - val_loss: 1.0307 - val_acc: 0.8410\n",
      "Learning rate = 0.001, decay = 0\n",
      "Epoch 100/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9778\n",
      "Epoch 00100: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0849 - acc: 0.9777 - val_loss: 0.8415 - val_acc: 0.8513\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 101/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9914\n",
      "Epoch 00101: val_loss improved from 0.73426 to 0.63545, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 81s 5ms/step - loss: 0.0298 - acc: 0.9914 - val_loss: 0.6355 - val_acc: 0.8890\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 102/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9942\n",
      "Epoch 00102: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0198 - acc: 0.9942 - val_loss: 0.6365 - val_acc: 0.8932\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 103/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9945\n",
      "Epoch 00103: val_loss improved from 0.63545 to 0.63057, saving model to ./modelWeights/weights.h5\n",
      "14829/14829 [==============================] - 81s 5ms/step - loss: 0.0199 - acc: 0.9944 - val_loss: 0.6306 - val_acc: 0.8975\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 104/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9961\n",
      "Epoch 00104: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.6440 - val_acc: 0.8975\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 105/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9966\n",
      "Epoch 00105: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0134 - acc: 0.9966 - val_loss: 0.6426 - val_acc: 0.8962\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 106/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9953\n",
      "Epoch 00106: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0129 - acc: 0.9953 - val_loss: 0.6514 - val_acc: 0.8987\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 107/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9963\n",
      "Epoch 00107: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0128 - acc: 0.9963 - val_loss: 0.6499 - val_acc: 0.8950\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 108/20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9964\n",
      "Epoch 00108: val_loss did not improve\n",
      "14829/14829 [==============================] - 76s 5ms/step - loss: 0.0102 - acc: 0.9964 - val_loss: 0.6513 - val_acc: 0.8975\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 109/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9970\n",
      "Epoch 00109: val_loss did not improve\n",
      "14829/14829 [==============================] - 76s 5ms/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.6706 - val_acc: 0.8975\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 110/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9970\n",
      "Epoch 00110: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.6689 - val_acc: 0.8956\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 111/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9972\n",
      "Epoch 00111: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0092 - acc: 0.9972 - val_loss: 0.6786 - val_acc: 0.8987\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 112/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9971\n",
      "Epoch 00112: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0090 - acc: 0.9971 - val_loss: 0.6823 - val_acc: 0.8981\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 113/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9972\n",
      "Epoch 00113: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0089 - acc: 0.9972 - val_loss: 0.6872 - val_acc: 0.8975\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 114/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9971\n",
      "Epoch 00114: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0083 - acc: 0.9971 - val_loss: 0.6971 - val_acc: 0.8987\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 115/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 00115: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.6870 - val_acc: 0.9005\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 116/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 00116: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.6905 - val_acc: 0.8999\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 117/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9975\n",
      "Epoch 00117: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.6960 - val_acc: 0.9035\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 118/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9973\n",
      "Epoch 00118: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.7055 - val_acc: 0.8993\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 119/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9976\n",
      "Epoch 00119: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.7150 - val_acc: 0.8987\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 120/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9980\n",
      "Epoch 00120: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.7039 - val_acc: 0.9005\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 121/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9976\n",
      "Epoch 00121: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.7043 - val_acc: 0.8987\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 122/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9976\n",
      "Epoch 00122: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.7128 - val_acc: 0.9011\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 123/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9976\n",
      "Epoch 00123: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.7101 - val_acc: 0.9017\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 124/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9980\n",
      "Epoch 00124: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0054 - acc: 0.9980 - val_loss: 0.7074 - val_acc: 0.9005\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 125/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9974\n",
      "Epoch 00125: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0060 - acc: 0.9974 - val_loss: 0.6889 - val_acc: 0.9017\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 126/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9978\n",
      "Epoch 00126: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0075 - acc: 0.9977 - val_loss: 0.7102 - val_acc: 0.9029\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 127/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9971\n",
      "Epoch 00127: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0081 - acc: 0.9971 - val_loss: 0.7330 - val_acc: 0.8993\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 128/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9977\n",
      "Epoch 00128: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0055 - acc: 0.9977 - val_loss: 0.7228 - val_acc: 0.8987\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 129/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9977\n",
      "Epoch 00129: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0065 - acc: 0.9977 - val_loss: 0.7296 - val_acc: 0.8981\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 130/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9975\n",
      "Epoch 00130: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0062 - acc: 0.9975 - val_loss: 0.7378 - val_acc: 0.8968\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 131/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9976\n",
      "Epoch 00131: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0065 - acc: 0.9976 - val_loss: 0.7236 - val_acc: 0.9011\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 132/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9982\n",
      "Epoch 00132: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.7269 - val_acc: 0.8981\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 133/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9976\n",
      "Epoch 00135: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0055 - acc: 0.9976 - val_loss: 0.7707 - val_acc: 0.8968\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 136/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9976\n",
      "Epoch 00136: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0064 - acc: 0.9976 - val_loss: 0.8237 - val_acc: 0.8914\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 137/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9976\n",
      "Epoch 00137: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.7565 - val_acc: 0.8975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 138/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 00138: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0063 - acc: 0.9977 - val_loss: 0.7526 - val_acc: 0.8993\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 139/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9979\n",
      "Epoch 00139: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0060 - acc: 0.9979 - val_loss: 0.9035 - val_acc: 0.8835\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 140/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9982\n",
      "Epoch 00140: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.7816 - val_acc: 0.9047\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 141/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9979\n",
      "Epoch 00141: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.7824 - val_acc: 0.9005\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 142/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9972\n",
      "Epoch 00142: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0079 - acc: 0.9972 - val_loss: 0.8033 - val_acc: 0.8981\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 143/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9982\n",
      "Epoch 00143: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0048 - acc: 0.9982 - val_loss: 0.7918 - val_acc: 0.8999\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 144/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9984\n",
      "Epoch 00144: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0048 - acc: 0.9984 - val_loss: 0.7696 - val_acc: 0.9017\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 145/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9979\n",
      "Epoch 00145: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0046 - acc: 0.9978 - val_loss: 0.7836 - val_acc: 0.9029\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 146/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9971\n",
      "Epoch 00146: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0094 - acc: 0.9971 - val_loss: 0.8014 - val_acc: 0.8968\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 147/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9978\n",
      "Epoch 00147: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.8019 - val_acc: 0.8962\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 148/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9978\n",
      "Epoch 00148: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.7569 - val_acc: 0.9053\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 149/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9980\n",
      "Epoch 00149: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0056 - acc: 0.9980 - val_loss: 0.7748 - val_acc: 0.8968\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 150/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9982\n",
      "Epoch 00150: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0045 - acc: 0.9982 - val_loss: 0.7763 - val_acc: 0.8987\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 151/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9982\n",
      "Epoch 00151: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0050 - acc: 0.9982 - val_loss: 0.7842 - val_acc: 0.8950\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 152/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9975\n",
      "Epoch 00152: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0073 - acc: 0.9974 - val_loss: 0.8177 - val_acc: 0.8932\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 153/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 00153: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0063 - acc: 0.9977 - val_loss: 0.7618 - val_acc: 0.8999\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 154/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9981\n",
      "Epoch 00154: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0050 - acc: 0.9981 - val_loss: 0.7366 - val_acc: 0.9011\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 155/20000\n",
      "  900/14829 [>.............................] - ETA: 1:09 - loss: 0.0027 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9987\n",
      "Epoch 00188: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.7244 - val_acc: 0.9005\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 189/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9985\n",
      "Epoch 00189: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0043 - acc: 0.9984 - val_loss: 0.7444 - val_acc: 0.9035\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 190/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9982\n",
      "Epoch 00190: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0048 - acc: 0.9982 - val_loss: 0.7451 - val_acc: 0.9023\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 191/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9986\n",
      "Epoch 00191: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.7407 - val_acc: 0.9053\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 192/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9983\n",
      "Epoch 00192: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0042 - acc: 0.9983 - val_loss: 0.7423 - val_acc: 0.9066\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 193/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9982\n",
      "Epoch 00193: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0041 - acc: 0.9982 - val_loss: 0.7367 - val_acc: 0.9084\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 194/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9979\n",
      "Epoch 00194: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0050 - acc: 0.9979 - val_loss: 0.7380 - val_acc: 0.9059\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 195/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9986\n",
      "Epoch 00195: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0033 - acc: 0.9986 - val_loss: 0.7605 - val_acc: 0.9023\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 196/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9984\n",
      "Epoch 00196: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.7629 - val_acc: 0.9047\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 197/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9980\n",
      "Epoch 00197: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0050 - acc: 0.9980 - val_loss: 0.7476 - val_acc: 0.9011\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 198/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9989\n",
      "Epoch 00198: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0047 - acc: 0.9989 - val_loss: 0.7539 - val_acc: 0.9029\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 199/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 00199: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0066 - acc: 0.9977 - val_loss: 0.7700 - val_acc: 0.8962\n",
      "Learning rate = 0.0001, decay = 1\n",
      "Epoch 200/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9982\n",
      "Epoch 00200: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.7697 - val_acc: 0.8944\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 201/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9984\n",
      "Epoch 00201: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0039 - acc: 0.9984 - val_loss: 0.7505 - val_acc: 0.8987\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 202/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9984\n",
      "Epoch 00202: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.7521 - val_acc: 0.8993\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 203/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9985\n",
      "Epoch 00203: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.7456 - val_acc: 0.9017\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 204/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9981\n",
      "Epoch 00204: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0044 - acc: 0.9981 - val_loss: 0.7407 - val_acc: 0.9023\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 205/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9982\n",
      "Epoch 00205: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0045 - acc: 0.9982 - val_loss: 0.7397 - val_acc: 0.9023\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 206/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9981\n",
      "Epoch 00206: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0054 - acc: 0.9980 - val_loss: 0.7372 - val_acc: 0.9035\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 207/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9984\n",
      "Epoch 00207: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0042 - acc: 0.9984 - val_loss: 0.7369 - val_acc: 0.9035\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 208/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9984\n",
      "Epoch 00208: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.7370 - val_acc: 0.9029\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 209/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9982\n",
      "Epoch 00209: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0041 - acc: 0.9982 - val_loss: 0.7392 - val_acc: 0.9047\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 210/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9983\n",
      "Epoch 00210: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.7364 - val_acc: 0.9047\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 211/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9984\n",
      "Epoch 00211: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.7335 - val_acc: 0.9029\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 212/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9986\n",
      "Epoch 00212: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.7321 - val_acc: 0.9035\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 213/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9985\n",
      "Epoch 00213: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 0.7336 - val_acc: 0.9041\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 214/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9984\n",
      "Epoch 00214: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0037 - acc: 0.9984 - val_loss: 0.7284 - val_acc: 0.9041\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 215/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9989\n",
      "Epoch 00215: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.7294 - val_acc: 0.9041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 216/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9982\n",
      "Epoch 00216: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0038 - acc: 0.9982 - val_loss: 0.7293 - val_acc: 0.9041\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 217/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9986\n",
      "Epoch 00217: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.7280 - val_acc: 0.9053\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 218/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9983\n",
      "Epoch 00218: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0044 - acc: 0.9983 - val_loss: 0.7320 - val_acc: 0.9035\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 219/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 00219: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0030 - acc: 0.9987 - val_loss: 0.7319 - val_acc: 0.9047\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 220/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9990\n",
      "Epoch 00220: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.7322 - val_acc: 0.9047\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 221/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 00221: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.7299 - val_acc: 0.9047\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 222/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9984\n",
      "Epoch 00222: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0036 - acc: 0.9984 - val_loss: 0.7293 - val_acc: 0.9041\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 223/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9987\n",
      "Epoch 00223: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.7293 - val_acc: 0.9035\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 224/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9984\n",
      "Epoch 00224: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0042 - acc: 0.9984 - val_loss: 0.7311 - val_acc: 0.9035\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 225/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9986\n",
      "Epoch 00225: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.7288 - val_acc: 0.9047\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 226/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9989\n",
      "Epoch 00226: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.7312 - val_acc: 0.9053\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 227/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 00227: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.7325 - val_acc: 0.9041\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 228/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9983\n",
      "Epoch 00228: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0042 - acc: 0.9983 - val_loss: 0.7348 - val_acc: 0.9053\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 229/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9986\n",
      "Epoch 00229: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.7330 - val_acc: 0.9047\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 230/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9984\n",
      "Epoch 00230: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0041 - acc: 0.9984 - val_loss: 0.7344 - val_acc: 0.9053\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 231/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9983\n",
      "Epoch 00231: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0034 - acc: 0.9983 - val_loss: 0.7333 - val_acc: 0.9047\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 232/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9983\n",
      "Epoch 00232: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0048 - acc: 0.9982 - val_loss: 0.7319 - val_acc: 0.9059\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 233/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9989\n",
      "Epoch 00233: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.7349 - val_acc: 0.9066\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 234/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9989\n",
      "Epoch 00234: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.7350 - val_acc: 0.9072\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 235/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9987\n",
      "Epoch 00235: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.7373 - val_acc: 0.9066\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 236/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9989\n",
      "Epoch 00236: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.7374 - val_acc: 0.9059\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 237/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9983\n",
      "Epoch 00237: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0035 - acc: 0.9983 - val_loss: 0.7323 - val_acc: 0.9059\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 238/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9989\n",
      "Epoch 00238: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.7337 - val_acc: 0.9072\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 239/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9990\n",
      "Epoch 00239: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.7334 - val_acc: 0.9066\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 240/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9984\n",
      "Epoch 00240: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 0.7317 - val_acc: 0.9084\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 241/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9986\n",
      "Epoch 00241: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.7383 - val_acc: 0.9072\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 242/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9989\n",
      "Epoch 00242: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.7372 - val_acc: 0.9066\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 243/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 00243: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 0.7301 - val_acc: 0.9059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 244/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 00244: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 0.7313 - val_acc: 0.9059\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 245/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9985\n",
      "Epoch 00245: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0035 - acc: 0.9985 - val_loss: 0.7323 - val_acc: 0.9066\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 246/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9987\n",
      "Epoch 00246: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 0.7332 - val_acc: 0.9084\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 247/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9985\n",
      "Epoch 00247: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0038 - acc: 0.9984 - val_loss: 0.7270 - val_acc: 0.9072\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 248/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9983\n",
      "Epoch 00248: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 0.7236 - val_acc: 0.9072\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 249/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9984\n",
      "Epoch 00249: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0041 - acc: 0.9984 - val_loss: 0.7243 - val_acc: 0.9072\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 250/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9989\n",
      "Epoch 00250: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.7245 - val_acc: 0.9078\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 251/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9989\n",
      "Epoch 00251: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.7263 - val_acc: 0.9090\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 252/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9982\n",
      "Epoch 00252: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0038 - acc: 0.9982 - val_loss: 0.7257 - val_acc: 0.9096\n",
      "Learning rate = 1e-05, decay = 2\n",
      "Epoch 253/20000\n",
      "14800/14829 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9988\n",
      "Epoch 00253: val_loss did not improve\n",
      "14829/14829 [==============================] - 77s 5ms/step - loss: 0.0025 - acc: 0.9988 - val_loss: 0.7250 - val_acc: 0.9084\n",
      "Execution time: 19634.51189517975 s\n"
     ]
    }
   ],
   "source": [
    "# store the starting time \n",
    "startTime = time.time()\n",
    "\n",
    "fitting_result = model.fit(X_train, y_train, epochs = nb_epochs, batch_size = batchSize, shuffle = True, callbacks = [earlyStopping, checkpointer, lr], validation_data = (X_validation, y_validation))\n",
    "\n",
    "# calculate the elapsed time\n",
    "elapsed = time.time()-startTime;\n",
    "print(\"Execution time: {0} s\".format(elapsed))\n",
    "\n",
    "# save model\n",
    "model.save(modelName2Save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the output (probabilistics) to classes\n",
    "def proba_to_class(a):\n",
    "    classCount = len(a[0])\n",
    "    print('proba_to_class:classCount={}'.format(classCount)) #M:\n",
    "    to_return = np.empty((0,classCount))\n",
    "    for row in a:\n",
    "        maxind = np.argmax(row)\n",
    "        to_return = np.vstack((to_return, [1 if i == maxind else 0 for i in range(classCount)]))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 51, 44, ..., 46,  6, 20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate metrics on test data with the last model \n",
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "y_result = model.predict(X_test)\n",
    "y_argmax = np.argmax(y_result, axis=1)\n",
    "y_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 51, 44, ..., 46,  6, 20])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_real = np.argmax(y_test, axis=1)\n",
    "y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AveragePrecision: 0.9433549323699938\n",
      "proba_to_class:classCount=123\n",
      "Accuracy: 0.9038776624795194\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics on test data with the last model \n",
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "\n",
    "map = average_precision_score(y_test, y_result, average='micro')\n",
    "print(\"AveragePrecision: {}\".format(map))\n",
    "\n",
    "accuracy = accuracy_score(y_test, proba_to_class(y_result))\n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload the best model with smallest validation loss and calculate metrics on test data\n",
    "# print(\"----- Loading best model from: {}  -------\".format(bestModelFilePath_val_loss))\n",
    "# model.load_weights(bestModelFilePath_val_loss)\n",
    "# y_result_bm = model.predict(X_test)\n",
    "# map_bm_val_loss = average_precision_score( y_test.data[y_test.start: y_test.end], y_result_bm, average='macro')\n",
    "# accuracy_bm_val_loss = accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result_bm))\n",
    "# print(\"AveragePrecision: {}\".format(map_bm_val_loss))\n",
    "# print(\"Accuracy: {}\".format(accuracy_bm_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload the best model with highest validation accuracy and calculate metrics on test data\n",
    "# print(\"----- Loading best model from: {}  -------\".format(bestModelFilePath_val_acc))\n",
    "# model.load_weights(bestModelFilePath_val_acc)\n",
    "# y_result_bm = model.predict(X_test)\n",
    "# map_bm_val_acc = average_precision_score( y_test.data[y_test.start: y_test.end], y_result_bm, average='macro')\n",
    "# accuracy_bm_val_acc = accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result_bm))\n",
    "# print(\"AveragePrecision: {}\".format(map_bm_val_acc))\n",
    "# print(\"Accuracy: {}\".format(accuracy_bm_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the results summery into an excel file\n",
    "# import log\n",
    "# log.logToXLS(logfileName, model, fitting_result, {'execution(s)':elapsed, 'map':map, 'accuracy':accuracy, 'map_bm_val_loss':map_bm_val_loss, 'accuracy_bm_val_loss':accuracy_bm_val_loss,'map_bm_val_acc':map_bm_val_acc, 'accuracy_bm_val_acc':accuracy_bm_val_acc, 'modelPyFile': modelPath})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
