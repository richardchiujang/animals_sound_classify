# -*- coding: utf-8 -*-
# this script is responsible for training the neural networks

from scipy import io
import pandas as pd
import numpy as np
import time
import pickle
# import os
import h5py
import sys, getopt
import datetime
from MapCallback import MapCallback


# 另一個常見的寫法是
import os
os.environ['CUDA_VISIBLE_DEVICES']='0,1'



nb_epochs = 20000 # number of epochs, should be high, the end of the learning process is controled by early stoping
es_patience = 100 # patience for early stoping 
batchSize = 100 # batch size for mini-batch training
hdf5path = '../birdclef_data/data_top468_nozero.hdf5' # training data generated by loadData.py
# modelPath = './model-AlexNet.py' # filename of the model to use (currently model-birdClef.py or model-AlexNet.py)
modelPath = 'InceptionV3'
logfileName = 'log.xls'
#scalerFilePath = '../birdclef_data/standardScaler_5000.pickle'
scalerFilePath = None
preTrainedModelWeightsPath = None # path and filename to pretrained network: if there is a pretrained network, we can load it and continue to train it
tensorflowBackend = True # set true if Keras has TensorFlow backend - this way we set TF not to allocate all the GPU memory

if (tensorflowBackend):
    import tensorflow as tf
    config = tf.ConfigProto()
    config.gpu_options.allow_growth=True
    sess = tf.Session(config=config)
    from keras import backend as K
    K.set_session(sess)

print('nb_epochs: %d, hdf5path: %s, scalerFilePath: %s' % (nb_epochs, hdf5path,scalerFilePath))


scaler = None
scaleData = None
# if a scaler file generated by loadData.py is given, than load it and define a scaler function that will be used later
if scalerFilePath is not None:
    scaler = pickle.load(open(scalerFilePath, 'rb'))
    # Can't use scaler.transform because it only supports 2d arrays.
    def scaleData(X):
        return (X-scaler.mean_)/scaler.scale_
    
    
from io_utils_mod import HDF5Matrix
f = h5py.File(hdf5path, 'r')
X = f.get('X')
y = f.get('y')
print("Shape of X: ")
print(X.shape)
dataSetLength = X.shape[0]
output_dim = y.shape[1] #len(y_train[0])
# test and validation splits
testSplit = 0.05 # 1%
validationSplit	= 0.1 #####0.05 # 5%
f.close()


print('dataSetLength={}'.format(dataSetLength))
print('output_dim={}'.format(output_dim))


# load training data
X_train = HDF5Matrix(hdf5path, 'X', 0, int(dataSetLength*(1-(testSplit+validationSplit))), normalizer = scaleData) 
y_train = HDF5Matrix(hdf5path, 'y', 0, int(dataSetLength*(1-(testSplit+validationSplit))))
# load validation data
X_validation = HDF5Matrix(hdf5path, 'X', int(dataSetLength*(1-(testSplit+validationSplit)))+1, int(dataSetLength*(1-testSplit)), normalizer = scaleData)
y_validation = HDF5Matrix(hdf5path, 'y', int(dataSetLength*(1-(testSplit+validationSplit)))+1, int(dataSetLength*(1-testSplit)))
# load test data
X_test = HDF5Matrix(hdf5path, 'X', int(dataSetLength*(1-testSplit))+1, dataSetLength, normalizer = scaleData)
y_test = HDF5Matrix(hdf5path, 'y', int(dataSetLength*(1-testSplit))+1, dataSetLength)

print("Shape of X_train after train-validation-test split:")
print('   X_train, y_train=', X_train.shape, y_train.shape, type(X_train), type(y_train))
print('   X_validation, y_validation=', X_validation.shape, y_validation.shape)
print('   X_test, y_test=', X_test.shape, y_test.shape)


from keras.applications.inception_v3 import InceptionV3
from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler
from keras.models import Model
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop
from keras.regularizers import l2
from keras.callbacks import EarlyStopping
# import keras.backend as K

# def preprocess_input(x):
#     x /= 255.
#     x -= 0.5
#     x *= 2.
#     return x


# lr decay schedule
def lr_schedule(epoch):
    """Learning Rate Schedule
    Learning rate is scheduled to be reduced after 80, 120epochs.
    Called automatically every epoch as part of callbacks during training.
    # Arguments
        epoch (int): The number of epochs
    # Returns
        lr (float32): learning rate
    """
#     lr = 1e-4
    lr = 1e-4
    decay = int(epoch / 10)
    if decay != 0:
        lr /= (10 ** decay)
    print('Learning rate = {}, decay = {}'.format(lr, decay))
    return lr


K.set_image_data_format('channels_first')
# from keras.utils.training_utils import multi_gpu_model
# gpu_count = 2

model = InceptionV3(include_top=False, weights=None, input_shape=(1,200,310), pooling='avg')
# model = InceptionV3(include_top=False, weights=None, input_shape=(1,200,310), pooling='avg')

# parallel_model = multi_gpu_model(model, gpus=gpu_count)

input_tensor = model.input
# input_tensor = get_input_at(node_index)
# build top
x = model.output
x = Dropout(.5)(x)
x = Dense(output_dim, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=x)

for layer in model.layers:
    layer.W_regularizer = l2(1e-2)
    layer.trainable = True

# call backs
if os.path.exists('./modelWeights/') is False:
    os.mkdir('./modelWeights/') #M:
checkpointer = ModelCheckpoint(filepath='./modelWeights/weights.h5', verbose=1, save_best_only=True)
lr = LearningRateScheduler(lr_schedule)
model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])
# parallel_model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])
# batch_size = batchSize * gpu_count


# model.summary()


# define callback functions
mapcallback	= MapCallback()
earlyStopping = EarlyStopping(monitor='val_loss', patience = es_patience) # early stoping


# training
#M: convert HDF5 array to Numpy array
X_train = np.array(X_train)
y_train = np.array(y_train)
X_validation = np.array(X_validation)
y_validation = np.array(y_validation)


print(X_validation.shape, y_validation.shape)
# y_validation[1]


# store the starting time 
startTime = time.time()

# fitting_result = model.fit(X_train, y_train, epochs = nb_epochs, batch_size = batchSize, callbacks = [earlyStopping, mapcallback, checkpointer_val_acc, checkpointer_val_loss,  checkpointer_val_map], shuffle = 'batch', validation_data = (X_validation, y_validation))
fitting_result = model.fit(X_train, y_train, epochs = nb_epochs, batch_size = batchSize, 
                           callbacks = [earlyStopping, checkpointer, lr], 
#                            callbacks = [checkpointer, lr], 
                           shuffle = 'batch', 
                           validation_data = (X_validation, y_validation))

# calculate the elapsed time
elapsed = time.time()-startTime;
print("Execution time: {0} s".format(elapsed))

# # save model
# model.save('trainModel_InceptionV3.h5')



# convert the output (probabilistics) to classes
def proba_to_class(a):
    classCount = len(a[0])
    to_return = np.empty((0,classCount))
    for row in a:
        maxind = np.argmax(row)
        to_return = np.vstack((to_return, [1 if i == maxind else 0 for i in range(classCount)]))
    return to_return






