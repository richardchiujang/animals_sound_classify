
# coding: utf-8

# 
# 

# ### 20180417: for bird_tw with 73 classes 

# In[1]:


from scipy import io
import pandas as pd
import numpy as np
import time
import pickle
import os
import h5py
import sys, getopt
import datetime
from io_utils_mod import HDF5Matrix
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt



# get_ipython().run_line_magic('matplotlib', 'inline')
# from MapCallback import MapCallback

os.environ['CUDA_VISIBLE_DEVICES']="0,1"


# In[2]:


hdf5path = '../data/all_data.hdf5' # training data generated by loadData.py
modelName2Save = 'trainModel_InceptionV3_1-v5_bird_tw.h5'


# In[3]:


modelPath = 'InceptionV3' #M: 沒用
logfileName = 'log.xls' #M: 沒用
#scalerFilePath = '../birdclef_data/standardScaler_5000.pickle'
scalerFilePath = None
preTrainedModelWeightsPath = None # path and filename to pretrained network: if there is a pretrained network, we can load it and continue to train it


print('hdf5path: %s, scalerFilePath: %s' % (hdf5path,scalerFilePath))


# In[4]:


scaler = None
scaleData = None
# if a scaler file generated by loadData.py is given, than load it and define a scaler function that will be used later
if scalerFilePath is not None:
    scaler = pickle.load(open(scalerFilePath, 'rb'))
    # Can't use scaler.transform because it only supports 2d arrays.
    def scaleData(X):
        return (X-scaler.mean_)/scaler.scale_


# In[5]:


f = h5py.File(hdf5path, 'r')
X = f.get('X')
y = f.get('y')
print("Shape of X:", X.shape)
dataSetLength = X.shape[0]
output_dim = y.shape[1] #len(y_train[0])
f.close()
print('dataSetLength={}'.format(dataSetLength))
print('output_dim={}'.format(output_dim))


# In[6]:

# keras.utils.io_utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None)
X = np.array(HDF5Matrix(hdf5path, 'X', 0, dataSetLength, normalizer = scaleData))
y = np.array(HDF5Matrix(hdf5path, 'y', 0, dataSetLength))

#M: vvv 再多 shuffle 幾次...
X, y = shuffle(X, y, random_state=4)
X, y = shuffle(X, y)
X, y = shuffle(X, y)
X, y = shuffle(X, y)


# In[7]:


X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 4, test_size = 0.1)
X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, random_state = 3, test_size = 0.1)

print('   X_train, y_train=', X_train.shape, y_train.shape, type(X_train), type(y_train))
print('   X_validation, y_validation=', X_validation.shape, y_validation.shape)
print('   X_test, y_test=', X_test.shape, y_test.shape)


# In[8]:


#M: 隨便抽張圖看看...
# i=1
#img = X_train[i].reshape((200,310))
# fig = plt.figure(figsize=(19,10))
# plt.tight_layout()
# plt.pcolormesh(img, cmap=plt.cm.binary)
# plt.show()


# ### vvvvv 將 AlexNet 換成 InceptionV3 Model vvvvv

# In[9]:

import tensorflow as tf
from keras.applications.inception_v3 import InceptionV3
from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler
from keras.models import Model
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop, Adam, Nadam #M: 用 Adam, Nadam 沒用.
from keras.regularizers import l2
from keras.callbacks import EarlyStopping
import keras.backend as K
from keras.utils import multi_gpu_model
# from keras.models import save_weights


tensorflowBackend = True # set true if Keras has TensorFlow backend - this way we set TF not to allocate all the GPU memory

if (tensorflowBackend):
    import tensorflow as tf
    config = tf.ConfigProto()
    config.gpu_options.allow_growth=True
    sess = tf.Session(config=config)
    from keras import backend as K
    K.set_session(sess)
# In[10]:


K.set_image_data_format('channels_first')
model = InceptionV3(include_top=False, weights=None, input_shape=(1,200,310), pooling='avg')

input_tensor = model.input
# build top
x = model.output
#M: 多加一層 128 dense layer
x = Dropout(.5)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(.5)(x)
x = Dense(output_dim, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=x)

parallel_model = multi_gpu_model(model, gpus=2)
parallel_model.compile(optimizer=Nadam(), loss='categorical_crossentropy', metrics=['accuracy'])
# model.compile(optimizer=Nadam(), loss='categorical_crossentropy', metrics=['accuracy'])


# In[11]:


# model.summary()


# ### ^^^^^ 將 AlexNet 換成 InceptionV3 Model ^^^^^

# In[12]:


nb_epochs = 10 # number of epochs, should be high, the end of the learning process is controled by early stoping
es_patience = 100 # patience for early stoping 
batchSize = 10 # batch size for mini-batch training


# In[13]:


# lr decay schedule
def lr_schedule(epoch):
    """Learning Rate Schedule
    Learning rate is scheduled to be reduced after 80, 120epochs.
    Called automatically every epoch as part of callbacks during training.
    # Arguments
        epoch (int): The number of epochs
    # Returns
        lr (float32): learning rate
    """

    lr = 1e-3
    decay = int(epoch / 50)
    if decay != 0:
        lr /= (10 ** decay)
    print('Learning rate = {}, decay = {}'.format(lr, decay))
    return lr


# In[14]:


# call backs
if os.path.exists('./modelWeights/') is False:
    os.mkdir('./modelWeights/') #M:
# checkpoint = ParallelModelCheckpoint('./modelWeights/weights.{epoch:02d}-{val_loss:.2f}.hdf5')  
checkpointer = ModelCheckpoint(filepath='./modelWeights/weights.{epoch:04d}-{val_loss:.2f}.hdf5', save_weights_only=True, verbose=1, save_best_only=True)

# checkpointer = ModelCheckpoint(filepath='./modelWeights/weights.h5', verbose=1, save_best_only=True)

lr = LearningRateScheduler(lr_schedule)


# In[15]:


# load model and compile it, we use RMSprop here, other optimizer algorithm should be tested
# execfile(modelPath)
# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')#, metrics=["accuracy"])

# print the model
# print("The following model is used: ")
# for layer in model.layers:
#     print("{} output shape: {}".format(layer.name, layer.output_shape))

# load pretrained model if it is set
# if preTrainedModelWeightsPath is not None:
#     model.load_weights(preTrainedModelWeightsPath)
#     print("Reloaded weights from: {}".format(preTrainedModelWeightsPath))

# define callback functions
# mapcallback = MapCallback()

earlyStopping = EarlyStopping(monitor='val_loss', patience = es_patience) # early stoping

# save best models based on accuracy, loss and MAP metrics
#bestModelFilePath_val_map = './modelWeights/best_val_map_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))
#bestModelFilePath_val_acc = './modelWeights/best_val_acc_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))
#bestModelFilePath_val_loss = './modelWeights/best_val_loss_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))


# bestModelFilePath_val_acc = './modelWeights/best_val_acc_{}.hdf5'.format(output_dim)
# bestModelFilePath_val_loss = './modelWeights/best_val_loss_{}.hdf5'.format(output_dim)
# bestModelFilePath_val_map = './modelWeights/best_val_map_{}.hdf5'.format(output_dim)
# checkpointer_val_acc = ModelCheckpoint(filepath = bestModelFilePath_val_acc, verbose = 1, monitor = 'val_acc', save_best_only = True)
# checkpointer_val_loss = ModelCheckpoint(filepath = bestModelFilePath_val_loss, verbose = 1, monitor = 'val_loss', save_best_only = True)
# checkpointer_val_map = ModelCheckpoint(filepath = bestModelFilePath_val_map, verbose = 1, monitor = 'val_map', mode = 'max', save_best_only = True)


# In[ ]:


# store the starting time 
startTime = time.time()

fitting_result = parallel_model.fit(X_train, y_train, epochs = nb_epochs, batch_size = batchSize, shuffle = True, 
                                    callbacks = [earlyStopping, checkpointer, lr], 
                                    # callbacks = [earlyStopping, lr],
                                    validation_data = (X_validation, y_validation))
# model.fit(X_train, y_train, epochs = nb_epochs, batch_size = batchSize, shuffle = True, callbacks = [earlyStopping, checkpointer, lr], # validation_data = (X_validation, y_validation))

# calculate the elapsed time
elapsed = time.time()-startTime;
print("Execution time: {0} s".format(elapsed))

# save model
# parallel_model.save(modelName2Save)
model.save(modelName2Save)


# In[18]:


# convert the output (probabilistics) to classes
def proba_to_class(a):
    classCount = len(a[0])
    print('proba_to_class:classCount={}'.format(classCount)) #M:
    to_return = np.empty((0,classCount))
    for row in a:
        maxind = np.argmax(row)
        to_return = np.vstack((to_return, [1 if i == maxind else 0 for i in range(classCount)]))
    return to_return


# In[19]:


# calculate metrics on test data with the last model 
from sklearn.metrics import average_precision_score, accuracy_score
y_result = model.predict(X_test)
y_argmax = np.argmax(y_result, axis=1)
y_argmax


# In[20]:


y_real = np.argmax(y_test, axis=1)
y_real


# In[21]:


# calculate metrics on test data with the last model 
from sklearn.metrics import average_precision_score, accuracy_score

map = average_precision_score(y_test, y_result, average='micro')
print("AveragePrecision: {}".format(map))

accuracy = accuracy_score(y_test, proba_to_class(y_result))
print("Accuracy: {}".format(accuracy))


# In[ ]:


# # reload the best model with smallest validation loss and calculate metrics on test data
# print("----- Loading best model from: {}  -------".format(bestModelFilePath_val_loss))
# model.load_weights(bestModelFilePath_val_loss)
# y_result_bm = model.predict(X_test)
# map_bm_val_loss = average_precision_score( y_test.data[y_test.start: y_test.end], y_result_bm, average='macro')
# accuracy_bm_val_loss = accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result_bm))
# print("AveragePrecision: {}".format(map_bm_val_loss))
# print("Accuracy: {}".format(accuracy_bm_val_loss))


# In[ ]:


# # reload the best model with highest validation accuracy and calculate metrics on test data
# print("----- Loading best model from: {}  -------".format(bestModelFilePath_val_acc))
# model.load_weights(bestModelFilePath_val_acc)
# y_result_bm = model.predict(X_test)
# map_bm_val_acc = average_precision_score( y_test.data[y_test.start: y_test.end], y_result_bm, average='macro')
# accuracy_bm_val_acc = accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result_bm))
# print("AveragePrecision: {}".format(map_bm_val_acc))
# print("Accuracy: {}".format(accuracy_bm_val_acc))


# In[ ]:


# # save the results summery into an excel file
# import log
# log.logToXLS(logfileName, model, fitting_result, {'execution(s)':elapsed, 'map':map, 'accuracy':accuracy, 'map_bm_val_loss':map_bm_val_loss, 'accuracy_bm_val_loss':accuracy_bm_val_loss,'map_bm_val_acc':map_bm_val_acc, 'accuracy_bm_val_acc':accuracy_bm_val_acc, 'modelPyFile': modelPath})

