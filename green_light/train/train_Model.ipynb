{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Birdsong classificatione in noisy environment with convolutional neural nets in Keras\n",
    "# Copyright (C) 2017 Báint Czeba, Bálint Pál Tóth (toth.b@tmit.bme.hu)\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>. (c) Balint Czeba, Balint Pal Toth\n",
    "# \n",
    "# Please cite the following paper if this code was useful for your research:\n",
    "# \n",
    "# Bálint Pál Tóth, Bálint Czeba,\n",
    "# \"Convolutional Neural Networks for Large-Scale Bird Song Classification in Noisy Environment\", \n",
    "# In: Working Notes of Conference and Labs of the Evaluation Forum, Évora, Portugália, 2016, p. 8\n",
    "\n",
    "# this script is responsible for training the neural networks\n",
    "\n",
    "from scipy import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "import sys, getopt\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    argv=sys.argv[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_epochs\t= 1 # number of epochs, should be high, the end of the learning process is controled by early stoping\n",
    "epochs = 20000\n",
    "# nb_epochs\t= 20000 # number of epochs, should be high, the end of the learning process is controled by early stoping\n",
    "es_patience\t= 100 # patience for early stoping \n",
    "batchSize\t= 1350 # batch size for mini-batch training\n",
    "# batchSize\t= 1 # batch size for mini-batch training\n",
    "###################################################################################################\n",
    "# hdf5path\t= '../birdclef_data/data_top999_nozero.hdf5' # training data generated by loadData.py\n",
    "hdf5path\t= '../birdclef_data/data_top468_nozero.hdf5' # training data generated by loadData.py\n",
    "##################################################################################################\n",
    "# modelPath\t= './model-AlexNet.py' # filename of the model to use (currently model-birdClef.py or model-AlexNet.py)\n",
    "modelPath\t= '../model-AlexNet.py' # filename of the model to use (currently model-birdClef.py or model-AlexNet.py)\n",
    "logfileName\t= 'log.xls'\n",
    "#scalerFilePath\t= '../birdclef_data/standardScaler_5000.pickle'\n",
    "scalerFilePath\t= None\n",
    "preTrainedModelWeightsPath = None # path and filename to pretrained network: if there is a pretrained network, we can load it and continue to train it\n",
    "# tensorflowBackend = False # set true if Keras has TensorFlow backend - this way we set TF not to allocate all the GPU memory\n",
    "tensorflowBackend = True # set true if Keras has TensorFlow backend - this way we set TF not to allocate all the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
    "\n",
    "if (tensorflowBackend):\n",
    "\timport tensorflow as tf\n",
    "\tconfig = tf.ConfigProto()\n",
    "\tconfig.gpu_options.allow_growth=True\n",
    "\tsess = tf.Session(config=config)\n",
    "\tfrom keras import backend as K\n",
    "\tK.set_session(sess)  #這裡有問題，沒這參數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 20000, hdf5path: ../birdclef_data/data_top468_nozero.hdf5, scalerFilePath: None\n"
     ]
    }
   ],
   "source": [
    "print('epochs: %d, hdf5path: %s, scalerFilePath: %s' % (epochs, hdf5path,scalerFilePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "scaleData = None\n",
    "# if a scaler file generated by loadData.py is given, than load it and define a scaler function that will be used later\n",
    "if scalerFilePath is not None:\n",
    "    scaler = pickle.load(open(scalerFilePath, 'rb'))\n",
    "    # Can't use scaler.transform because it only supports 2d arrays.\n",
    "    def scaleData(X):\n",
    "        return (X-scaler.mean_)/scaler.scale_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: \n",
      "(5530, 1, 200, 310)\n"
     ]
    }
   ],
   "source": [
    "from io_utils_mod import HDF5Matrix\n",
    "f = h5py.File(hdf5path, 'r')\n",
    "X = f.get('X')\n",
    "y = f.get('y')\n",
    "print(\"Shape of X: \")\n",
    "print(X.shape)\n",
    "dataSetLength\t= X.shape[0]\n",
    "output_dim\t= y.shape[1] #len(y_train[0])\n",
    "# test and validation splits\n",
    "testSplit\t= 0.01 # 1%\n",
    "validationSplit\t= 0.05 # 5%\n",
    "# validationSplit\t= 0.1 # 5%\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "X_train = HDF5Matrix(hdf5path, 'X', 0, int(dataSetLength*(1-(testSplit+validationSplit))), normalizer = scaleData) \n",
    "y_train = HDF5Matrix(hdf5path, 'y', 0, int(dataSetLength*(1-(testSplit+validationSplit))))\n",
    "# load validation data\n",
    "X_validation = HDF5Matrix(hdf5path, 'X', int(dataSetLength*(1-(testSplit+validationSplit)))+1, \n",
    "                          int(dataSetLength*(1-testSplit)), normalizer = scaleData)\n",
    "y_validation = HDF5Matrix(hdf5path, 'y', int(dataSetLength*(1-(testSplit+validationSplit)))+1, \n",
    "                          int(dataSetLength*(1-testSplit)))\n",
    "# load test data\n",
    "X_test = HDF5Matrix(hdf5path, 'X', int(dataSetLength*(1-testSplit))+1, dataSetLength, normalizer = scaleData)\n",
    "y_test = HDF5Matrix(hdf5path, 'y', int(dataSetLength*(1-testSplit))+1, dataSetLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"y\": shape (5530, 468), type \"<f4\">"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train after train-validation-test split:\n",
      "(5198, 1, 200, 310)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train after train-validation-test split:\")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ..., \n",
       "        [ 0.23109023,  1.11156821,  0.01434718, ...,  1.48071444,\n",
       "          1.52842546,  1.34299016],\n",
       "        [ 0.14459638,  0.99199986,  0.38565734, ...,  0.09228765,\n",
       "          1.13456285, -0.36999109],\n",
       "        [ 1.06557512,  0.71072274,  0.93754435, ..., -0.67539954,\n",
       "          0.2943573 ,  0.78111243]]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D , MaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "# from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from MapCallback import MapCallback\n",
    "# from keras.layers.convolutional import Convolution2D,  MaxPooling2D \n",
    "\n",
    "# from theano import ifelse\n",
    "# Strongly AlexNet based convolutional neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# data_format=\"channels_first\"\n",
    "input_shape = (1,200,310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?Conv2D\n",
    "# Init signature: Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), \n",
    "#                        activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "#                        kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "#                        bias_constraint=None, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 卷积层一\n",
    "# model.add(Conv2D(32, kernel_size = (5, 5), strides = (1, 1), padding = 'same', activation = 'relu', input_shape = (1, 28, 28)))\n",
    "# #model.add(Convolution2D(\n",
    "# #    nb_filter=32,\n",
    "# #    nb_row=5,\n",
    "# #    nb_col=5,\n",
    "# #    border_mode='same',     # Padding method\n",
    "# #    dim_ordering='th',    # 采用 theano 的 input 格式  \n",
    "# #    input_shape=(1,         # channels\n",
    "# #                 28, 28,)    # height & width\n",
    "# #))\n",
    "# #model.add(Activation('relu'))\n",
    "\n",
    "# 作者：Ledestin\n",
    "# 链接：https://www.jianshu.com/p/ddf5ed31e44c\n",
    "# 來源：简书\n",
    "# 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_26: expected ndim=4, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-d4d5ef9efeb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#                 ))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m310\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    490\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    491\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    470\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_26: expected ndim=4, found ndim=2"
     ]
    }
   ],
   "source": [
    "# convolutional layer\n",
    "# model.add(Conv2D(input_shape=(1,200,310), \n",
    "#                  data_format=\"channels_first\",\n",
    "#                  nb_filter=48*2,\n",
    "#                  nb_row=16,\n",
    "#                  nb_col=16,\n",
    "#                  border_mode='valid',\n",
    "#                  init='glorot_normal', #glorot_normal lecun_uniform he_uniform\n",
    "#                  activation='relu'\n",
    "#                 ))\n",
    "\n",
    "model.add(Conv2D(96, kernel_size = (5,5),  padding = 'same', activation = 'relu', input_shape = (-1,1,200,310)))\n",
    "\n",
    "model.add(BatchNormalization())                        \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(nb_filter=128*2,\n",
    "                        nb_row=3,\n",
    "                        nb_col=3,\n",
    "                        border_mode='valid',\n",
    "                        init='lecun_uniform', #glorot_normal lecun_uniform he_uniform\n",
    "                        activation='relu'\n",
    "                        ))  \n",
    "model.add(BatchNormalization())                        \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(nb_filter=192*2,\n",
    "                        nb_row=3,\n",
    "                        nb_col=3,\n",
    "                        border_mode='same',\n",
    "                        init='lecun_uniform', #glorot_normal lecun_uniform he_uniform\n",
    "                        activation='relu'\n",
    "                        ))  \n",
    "\n",
    "model.add(Conv2D(nb_filter=192*2,\n",
    "                        nb_row=3,\n",
    "                        nb_col=3,\n",
    "                        border_mode='same',\n",
    "                        init='lecun_uniform', #glorot_normal lecun_uniform he_uniform\n",
    "                        activation='relu'\n",
    "                        ))  \n",
    "\n",
    "model.add(Conv2D(nb_filter=128*2,\n",
    "                        nb_row=3,\n",
    "                        nb_col=3,\n",
    "                        border_mode='same',\n",
    "                        init='lecun_uniform', #glorot_normal lecun_uniform he_uniform\n",
    "                        activation='relu'\n",
    "                        ))  \n",
    "\n",
    "model.add(BatchNormalization())                        \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# dense layers                  \n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(output_dim = output_dim, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and compile it, we use RMSprop here, other optimizer algorithm should be tested\n",
    "# execfile(modelPath)\n",
    "# exec(open('model-AlexNet.py', encoding=\"utf-8\").read())\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')#, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the model\n",
    "print(\"The following model is used: \")\n",
    "for layer in model.layers:\n",
    "    print(\"{} output shape: {}\".format(layer.name, layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model if it is set\n",
    "if preTrainedModelWeightsPath is not None:\n",
    "    model.load_weights(preTrainedModelWeightsPath)\n",
    "    print(\"Reloaded weights from: {}\".format(preTrainedModelWeightsPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback functions\n",
    "mapcallback\t= MapCallback()\n",
    "earlyStopping\t= EarlyStopping(monitor='val_loss', patience = es_patience) # early stoping\n",
    "# save best models based on accuracy, loss and MAP metrics\n",
    "#bestModelFilePath_val_map\t= './modelWeights/best_val_map_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))\n",
    "#bestModelFilePath_val_acc\t= './modelWeights/best_val_acc_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))\n",
    "#bestModelFilePath_val_loss\t= './modelWeights/best_val_loss_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))\n",
    "bestModelFilePath_val_acc\t= './modelWeights/best_val_acc_{}.hdf5'.format(output_dim)\n",
    "bestModelFilePath_val_loss\t= './modelWeights/best_val_loss_{}.hdf5'.format(output_dim)\n",
    "bestModelFilePath_val_map\t= './modelWeights/best_val_map_{}.hdf5'.format(output_dim)\n",
    "checkpointer_val_acc\t= ModelCheckpoint(filepath = bestModelFilePath_val_acc, verbose = 1, monitor = 'val_acc', save_best_only = True)\n",
    "checkpointer_val_loss\t= ModelCheckpoint(filepath = bestModelFilePath_val_loss, verbose = 1, monitor = 'val_loss', save_best_only = True)\n",
    "checkpointer_val_map\t= ModelCheckpoint(filepath = bestModelFilePath_val_map, verbose = 1, monitor = 'val_map', mode = 'max', save_best_only = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store the starting time \n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "fitting_result = model.fit(X_train, y_train, epochs = epochs, batch_size = batchSize,\n",
    "                            callbacks = [earlyStopping,\n",
    "                                         mapcallback,\n",
    "                                         checkpointer_val_acc, \n",
    "                                         checkpointer_val_loss,  \n",
    "                                         checkpointer_val_map\n",
    "                                        ], \n",
    "                            shuffle = None, \n",
    "                            validation_data = (X_validation, y_validation)\n",
    "                           )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate the elapsed time\n",
    "elapsed = time.time()-startTime;\n",
    "print(\"Execution time: {0} s\".format(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?model.fit\n",
    "# Signature: model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n",
    "# validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, \n",
    "# validation_steps=None, **kwargs)\n",
    "# Docstring:\n",
    "# Trains the model for a fixed number of epochs (iterations on a dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store the starting time \n",
    "# startTime = time.time()\n",
    "\n",
    "# # # fitting_result = model.fit(X_train, y_train, epochs = epochs, batch_size = batchSize, callbacks = [earlyStopping, mapcallback, checkpointer_val_acc, checkpointer_val_loss,  checkpointer_val_map], shuffle = 'batch', validation_data = (X_validation, y_validation))\n",
    "# fitting_result = model.fit(X_train, y_train, epochs = epochs, batch_size = batchSize, \n",
    "#                            callbacks = [earlyStopping\n",
    "# #                                         checkpointer, \n",
    "# #                                         lr\n",
    "#                                        ], shuffle = 'batch'\n",
    "# #                            validation_data = (X_validation, y_validation)\n",
    "#                           )\n",
    "\n",
    "# # calculate the elapsed time\n",
    "# elapsed = time.time()-startTime;\n",
    "# print(\"Execution time: {0} s\".format(elapsed))\n",
    "\n",
    "# # save model\n",
    "# model.save('train_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate the elapsed time\n",
    "# elapsed = time.time()-startTime;\n",
    "# print(\"Execution time: {0} s\".format(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the output (probabilistics) to classes\n",
    "def proba_to_class(a):\n",
    "    classCount\t= len(a[0])\n",
    "    to_return\t= np.empty((0,classCount))\n",
    "    for row in a:\n",
    "        maxind\t= np.argmax(row)\n",
    "        to_return = np.vstack((to_return, [1 if i == maxind else 0 for i in range(classCount)]))\n",
    "    return to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics on test data with the last model \n",
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "y_result\t= model.predict(X_test)\n",
    "map\t\t= average_precision_score( y_test.data[y_test.start: y_test.end], y_result, average='micro')\n",
    "accuracy\t= accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result))\n",
    "print(\"AveragePrecision: {}\".format(map))\n",
    "print(\"Accuracy: {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the best model with smallest validation loss and calculate metrics on test data\n",
    "print(\"----- Loading best model from: {}  -------\".format(bestModelFilePath_val_loss))\n",
    "model.load_weights(bestModelFilePath_val_loss)\n",
    "y_result_bm\t\t= model.predict(X_test)\n",
    "map_bm_val_loss\t\t= average_precision_score( y_test.data[y_test.start: y_test.end], y_result_bm, average='macro')\n",
    "accuracy_bm_val_loss\t= accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result_bm))\n",
    "print(\"AveragePrecision: {}\".format(map_bm_val_loss))\n",
    "print(\"Accuracy: {}\".format(accuracy_bm_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the best model with highest validation accuracy and calculate metrics on test data\n",
    "print(\"----- Loading best model from: {}  -------\".format(bestModelFilePath_val_acc))\n",
    "model.load_weights(bestModelFilePath_val_acc)\n",
    "y_result_bm\t\t= model.predict(X_test)\n",
    "map_bm_val_acc\t\t= average_precision_score( y_test.data[y_test.start: y_test.end], y_result_bm, average='macro')\n",
    "accuracy_bm_val_acc\t= accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result_bm))\n",
    "print(\"AveragePrecision: {}\".format(map_bm_val_acc))\n",
    "print(\"Accuracy: {}\".format(accuracy_bm_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results summery into an excel file\n",
    "import log\n",
    "log.logToXLS(logfileName, model, fitting_result, {'execution(s)':elapsed, 'map':map, 'accuracy':accuracy, 'map_bm_val_loss':map_bm_val_loss, 'accuracy_bm_val_loss':accuracy_bm_val_loss,'map_bm_val_acc':map_bm_val_acc, 'accuracy_bm_val_acc':accuracy_bm_val_acc, 'modelPyFile': modelPath})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
