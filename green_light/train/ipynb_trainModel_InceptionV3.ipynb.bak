{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# this script is responsible for training the neural networks\n",
    "\n",
    "from scipy import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "import sys, getopt\n",
    "import datetime\n",
    "from MapCallback import MapCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_epochs: 2, hdf5path: ../birdclef_data/data_top468_nozero.hdf5, scalerFilePath: None\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2 # number of epochs, should be high, the end of the learning process is controled by early stoping\n",
    "es_patience = 100 # patience for early stoping \n",
    "batchSize = 10 # batch size for mini-batch training\n",
    "hdf5path = '../birdclef_data/data_top468_nozero.hdf5' # training data generated by loadData.py\n",
    "# modelPath = './model-AlexNet.py' # filename of the model to use (currently model-birdClef.py or model-AlexNet.py)\n",
    "modelPath = 'InceptionV3'\n",
    "logfileName = 'log.xls'\n",
    "#scalerFilePath = '../birdclef_data/standardScaler_5000.pickle'\n",
    "scalerFilePath = None\n",
    "preTrainedModelWeightsPath = None # path and filename to pretrained network: if there is a pretrained network, we can load it and continue to train it\n",
    "tensorflowBackend = False # set true if Keras has TensorFlow backend - this way we set TF not to allocate all the GPU memory\n",
    "\n",
    "if (tensorflowBackend):\n",
    "    import tensorflow as tf\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    from keras import backend as K\n",
    "    K.set_session(sess)\n",
    "\n",
    "print('nb_epochs: %d, hdf5path: %s, scalerFilePath: %s' % (nb_epochs, hdf5path,scalerFilePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "scaleData = None\n",
    "# if a scaler file generated by loadData.py is given, than load it and define a scaler function that will be used later\n",
    "if scalerFilePath is not None:\n",
    "    scaler = pickle.load(open(scalerFilePath, 'rb'))\n",
    "    # Can't use scaler.transform because it only supports 2d arrays.\n",
    "    def scaleData(X):\n",
    "        return (X-scaler.mean_)/scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: \n",
      "(5530, 1, 200, 310)\n"
     ]
    }
   ],
   "source": [
    "from io_utils_mod import HDF5Matrix\n",
    "f = h5py.File(hdf5path, 'r')\n",
    "X = f.get('X')\n",
    "y = f.get('y')\n",
    "print(\"Shape of X: \")\n",
    "print(X.shape)\n",
    "dataSetLength = X.shape[0]\n",
    "output_dim = y.shape[1] #len(y_train[0])\n",
    "# test and validation splits\n",
    "testSplit = 0.01 # 1%\n",
    "validationSplit\t= 0.05 #####0.05 # 5%\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataSetLength=5530\n",
      "output_dim=468\n"
     ]
    }
   ],
   "source": [
    "print('dataSetLength={}'.format(dataSetLength))\n",
    "print('output_dim={}'.format(output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train after train-validation-test split:\n",
      "   X_train, y_train= (5198, 1, 200, 310) (5198, 468) <class 'io_utils_mod.HDF5Matrix'> <class 'io_utils_mod.HDF5Matrix'>\n",
      "   X_validation, y_validation= (275, 1, 200, 310) (275, 468)\n",
      "   X_test, y_test= (55, 1, 200, 310) (55, 468)\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "X_train = HDF5Matrix(hdf5path, 'X', 0, int(dataSetLength*(1-(testSplit+validationSplit))), normalizer = scaleData) \n",
    "y_train = HDF5Matrix(hdf5path, 'y', 0, int(dataSetLength*(1-(testSplit+validationSplit))))\n",
    "# load validation data\n",
    "X_validation = HDF5Matrix(hdf5path, 'X', int(dataSetLength*(1-(testSplit+validationSplit)))+1, int(dataSetLength*(1-testSplit)), normalizer = scaleData)\n",
    "y_validation = HDF5Matrix(hdf5path, 'y', int(dataSetLength*(1-(testSplit+validationSplit)))+1, int(dataSetLength*(1-testSplit)))\n",
    "# load test data\n",
    "X_test = HDF5Matrix(hdf5path, 'X', int(dataSetLength*(1-testSplit))+1, dataSetLength, normalizer = scaleData)\n",
    "y_test = HDF5Matrix(hdf5path, 'y', int(dataSetLength*(1-testSplit))+1, dataSetLength)\n",
    "\n",
    "print(\"Shape of X_train after train-validation-test split:\")\n",
    "print('   X_train, y_train=', X_train.shape, y_train.shape, type(X_train), type(y_train))\n",
    "print('   X_validation, y_validation=', X_validation.shape, y_validation.shape)\n",
    "print('   X_test, y_test=', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vvvvv 將 AlexNet 換成 InceptionV3 Model vvvvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "\n",
    "# def preprocess_input(x):\n",
    "#     x /= 255.\n",
    "#     x -= 0.5\n",
    "#     x *= 2.\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr decay schedule\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-4\n",
    "    decay = int(epoch / 10)\n",
    "    if decay != 0:\n",
    "        lr /= (10 ** decay)\n",
    "    print('Learning rate = {}, decay = {}'.format(lr, decay))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "model = InceptionV3(include_top=False, weights=None, input_shape=(1,200,310), pooling='avg')\n",
    "\n",
    "input_tensor = model.input\n",
    "# build top\n",
    "x = model.output\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(output_dim, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=x)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.W_regularizer = l2(1e-2)\n",
    "    layer.trainable = True\n",
    "\n",
    "# call backs\n",
    "if os.path.exists('./modelWeights/') is False:\n",
    "    os.mkdir('./modelWeights/') #M:\n",
    "checkpointer = ModelCheckpoint(filepath='./modelWeights/weights.h5', verbose=1, save_best_only=True)\n",
    "lr = LearningRateScheduler(lr_schedule)\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^^^^^ 將 AlexNet 換成 InceptionV3 Model ^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and compile it, we use RMSprop here, other optimizer algorithm should be tested\n",
    "# execfile(modelPath)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')#, metrics=[\"accuracy\"])\n",
    "\n",
    "# print the model\n",
    "# print(\"The following model is used: \")\n",
    "# for layer in model.layers:\n",
    "#     print(\"{} output shape: {}\".format(layer.name, layer.output_shape))\n",
    "\n",
    "# load pretrained model if it is set\n",
    "# if preTrainedModelWeightsPath is not None:\n",
    "#     model.load_weights(preTrainedModelWeightsPath)\n",
    "#     print(\"Reloaded weights from: {}\".format(preTrainedModelWeightsPath))\n",
    "\n",
    "# define callback functions\n",
    "mapcallback\t= MapCallback()\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience = es_patience) # early stoping\n",
    "\n",
    "# save best models based on accuracy, loss and MAP metrics\n",
    "#bestModelFilePath_val_map = './modelWeights/best_val_map_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))\n",
    "#bestModelFilePath_val_acc = './modelWeights/best_val_acc_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))\n",
    "#bestModelFilePath_val_loss = './modelWeights/best_val_loss_{}_{}.hdf5'.format(output_dim, datetime.datetime.now().strftime('%Y-%m-%d-%M-%S'))\n",
    "\n",
    "\n",
    "# bestModelFilePath_val_acc = './modelWeights/best_val_acc_{}.hdf5'.format(output_dim)\n",
    "# bestModelFilePath_val_loss = './modelWeights/best_val_loss_{}.hdf5'.format(output_dim)\n",
    "# bestModelFilePath_val_map = './modelWeights/best_val_map_{}.hdf5'.format(output_dim)\n",
    "# checkpointer_val_acc = ModelCheckpoint(filepath = bestModelFilePath_val_acc, verbose = 1, monitor = 'val_acc', save_best_only = True)\n",
    "# checkpointer_val_loss = ModelCheckpoint(filepath = bestModelFilePath_val_loss, verbose = 1, monitor = 'val_loss', save_best_only = True)\n",
    "# checkpointer_val_map = ModelCheckpoint(filepath = bestModelFilePath_val_map, verbose = 1, monitor = 'val_map', mode = 'max', save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "#M: convert HDF5 array to Numpy array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_validation = np.array(X_validation)\n",
    "y_validation = np.array(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275, 1, 200, 310) (275, 468)\n"
     ]
    }
   ],
   "source": [
    "print(X_validation.shape, y_validation.shape)\n",
    "# y_validation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5198 samples, validate on 275 samples\n",
      "Learning rate = 0.0001, decay = 0\n",
      "Epoch 1/2\n",
      "5190/5198 [============================>.] - ETA: 0s - loss: 5.7775 - acc: 0.0202\n",
      "Epoch 00001: val_loss did not improve\n",
      "5198/5198 [==============================] - 56s 11ms/step - loss: 5.7768 - acc: 0.0202 - val_loss: 6.7375 - val_acc: 0.0000e+00\n",
      "Learning rate = 0.0001, decay = 0\n",
      "Epoch 2/2\n",
      "5190/5198 [============================>.] - ETA: 0s - loss: 5.7912 - acc: 0.0200\n",
      "Epoch 00002: val_loss did not improve\n",
      "5198/5198 [==============================] - 56s 11ms/step - loss: 5.7907 - acc: 0.0200 - val_loss: 6.7961 - val_acc: 0.0000e+00\n",
      "Execution time: 111.26403903961182 s\n"
     ]
    }
   ],
   "source": [
    "# store the starting time \n",
    "startTime = time.time()\n",
    "\n",
    "# fitting_result = model.fit(X_train, y_train, epochs = nb_epochs, batch_size = batchSize, callbacks = [earlyStopping, mapcallback, checkpointer_val_acc, checkpointer_val_loss,  checkpointer_val_map], shuffle = 'batch', validation_data = (X_validation, y_validation))\n",
    "fitting_result = model.fit(X_train, y_train, epochs = nb_epochs, batch_size = batchSize, \n",
    "                           callbacks = [earlyStopping, checkpointer, lr], \n",
    "#                            callbacks = [checkpointer, lr], \n",
    "                           shuffle = 'batch', \n",
    "                           validation_data = (X_validation, y_validation))\n",
    "\n",
    "# calculate the elapsed time\n",
    "elapsed = time.time()-startTime;\n",
    "print(\"Execution time: {0} s\".format(elapsed))\n",
    "\n",
    "# # save model\n",
    "# model.save('trainModel_InceptionV3.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('trainModel_InceptionV3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the output (probabilistics) to classes\n",
    "def proba_to_class(a):\n",
    "    classCount = len(a[0])\n",
    "    to_return = np.empty((0,classCount))\n",
    "    for row in a:\n",
    "        maxind = np.argmax(row)\n",
    "        to_return = np.vstack((to_return, [1 if i == maxind else 0 for i in range(classCount)]))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HDF5Matrix' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-16837bd2f50a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate metrics on test data with the last model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproba_to_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1781\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HDF5Matrix' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "# calculate metrics on test data with the last model \n",
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "y_result = model.predict(X_test)\n",
    "map = average_precision_score( y_test.data[y_test.start: y_test.end], y_result, average='micro')\n",
    "accuracy = accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result))\n",
    "print(\"AveragePrecision: {}\".format(map))\n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the best model with smallest validation loss and calculate metrics on test data\n",
    "print(\"----- Loading best model from: {}  -------\".format(bestModelFilePath_val_loss))\n",
    "model.load_weights(bestModelFilePath_val_loss)\n",
    "y_result_bm = model.predict(X_test)\n",
    "map_bm_val_loss = average_precision_score( y_test.data[y_test.start: y_test.end], y_result_bm, average='macro')\n",
    "accuracy_bm_val_loss = accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result_bm))\n",
    "print(\"AveragePrecision: {}\".format(map_bm_val_loss))\n",
    "print(\"Accuracy: {}\".format(accuracy_bm_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the best model with highest validation accuracy and calculate metrics on test data\n",
    "print(\"----- Loading best model from: {}  -------\".format(bestModelFilePath_val_acc))\n",
    "model.load_weights(bestModelFilePath_val_acc)\n",
    "y_result_bm = model.predict(X_test)\n",
    "map_bm_val_acc = average_precision_score( y_test.data[y_test.start: y_test.end], y_result_bm, average='macro')\n",
    "accuracy_bm_val_acc = accuracy_score(y_test.data[y_test.start: y_test.end], proba_to_class(y_result_bm))\n",
    "print(\"AveragePrecision: {}\".format(map_bm_val_acc))\n",
    "print(\"Accuracy: {}\".format(accuracy_bm_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results summery into an excel file\n",
    "import log\n",
    "log.logToXLS(logfileName, model, fitting_result, {'execution(s)':elapsed, 'map':map, 'accuracy':accuracy, 'map_bm_val_loss':map_bm_val_loss, 'accuracy_bm_val_loss':accuracy_bm_val_loss,'map_bm_val_acc':map_bm_val_acc, 'accuracy_bm_val_acc':accuracy_bm_val_acc, 'modelPyFile': modelPath})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在準備好 X_train ....後加一個 cell\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "i=3\n",
    "img = X_train[i].reshape((200,310))\n",
    "print(X_train[i].shape, img.shape)\n",
    "plt.imshow(img, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
